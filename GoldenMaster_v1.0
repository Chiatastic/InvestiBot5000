import numpy as np
import onnxruntime
from huggingface_hub import hf_hub_download
import imaplib
import email
from email.header import decode_header
import time
import os
import re
from datetime import datetime
import pytz
from PyPDF2 import PdfReader
import docx
from bs4 import BeautifulSoup
import requests
import json
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from PIL import Image
from pyzbar.pyzbar import decode as qr_decode
import pytesseract
from urllib.parse import urlparse, urljoin
import concurrent.futures
from collections import defaultdict
import msal
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urlextract import URLExtract
import shutil
import hashlib
import io

# ==========================================
# ============ PHISHING SETTINGS ============
# ==========================================

# List of phishing-related phrases to search for
PHISH_PHRASES = [
    "gift card",
    "bitcoin",
    "kindly",
    "voicemail",
    "shared a file",
    "urgent",
    "login",
    "sign in",
    "sign in with a different microsoft account",
    "enter password",
    "norton",
    "password",
    "mcafee",
    "charge",
    "suspicious",
    "authorized",
    "about to expire",
    "locked for security",
    "gift card",
    "geek"
    
    
]

def search_phish_phrases(text):
    """
    Searches for any phishing phrases within the provided text.

    Args:
        text (str): The text to search within.

    Returns:
        bool: True if any phishing phrase is found, False otherwise.
    """
    text_lower = text.lower()
    for phrase in PHISH_PHRASES:
        if phrase.lower() in text_lower:
            return True
    return False

# ==========================================
# ============ ONNX MODEL SETUP ============
# ==========================================

REPO_ID = "pirocheto/phishing-url-detection"
FILENAME = "model.onnx"
model_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)
session = onnxruntime.InferenceSession(model_path, providers=["CPUExecutionProvider"])

def predict_phishing(url):
    urls = np.array([url], dtype="str")
    probabilities = session.run(None, {"inputs": urls})[1]
    phishing_probability = probabilities[0][1] * 100  # Convert to percentage
    return phishing_probability

# ==========================================
# ============ EMAIL SETTINGS ===============
# ==========================================

# Outlook login credentials
# Outlook login credentials
USERNAME = '******@outlook.com'
PASSWORD = '*******'  # Replace with your app password
IMAP_SERVER = 'outlook.office365.com'
SMTP_SERVER = 'smtp.office365.com'
SMTP_PORT = 587

# VirusTotal API key
VT_API_KEY = '**************'  # Replace with your actual API key
VT_API_URL = "https://www.virustotal.com/api/v3"

# Microsoft API credentials
CLIENT_ID = '*************'        # Replace with your Application (client) ID
CLIENT_SECRET = '************8'  # Replace with your Client Secret Value
TENANT_ID = '***************'        # Replace with your Tenant ID

# Excluded Domains for VirusTotal Scanning, Screenshot Capture, and ONNX Model
EXCLUDED_DOMAINS = ['mimecast.com', 'fireeye.com', 'westernexp.com']  # Added 'westernexp.com'
EXCLUDED_MICROSOFT_DOMAINS = ['microsoft.com', 'live.com', 'microsoftonline.com', 'outlook.com', 'office.com']
ALLOWED_DOMAINS = ['sharepoint.com']

# Trusted Domains to Identify Legitimate Websites
TRUSTED_DOMAINS = [
    'google.com',
    'microsoft.com',
    'facebook.com',
    'apple.com',
    'amazon.com',
    'twitter.com',
    'linkedin.com',
    'github.com',
    'instagram.com',
    'youtube.com',
    'yahoo.com',
    'reddit.com',
    'wordpress.com',
    'wikipedia.org',
    'netflix.com',
    'zoom.us',
    'dropbox.com',
    'paypal.com',
    'ebay.com',
    'tiktok.com'
]

MAX_RETRIES = 3  # Maximum number of retries for taking screenshots
MAX_LINKS_PER_DOMAIN = 4  # Maximum number of links to crawl per domain
CRAWL_LINK_LIMIT = 10  # Maximum number of links to crawl overall

# ==========================================
# ============ HELPER FUNCTIONS ============
# ==========================================

def is_trusted_domain(domain):
    """
    Checks if a domain is in the list of trusted domains.

    Args:
        domain (str): The domain to check.

    Returns:
        bool: True if the domain is trusted, False otherwise.
    """
    return any(trusted in domain for trusted in TRUSTED_DOMAINS)

def is_excluded_domain(link):
    parsed_url = urlparse(link)
    domain = parsed_url.netloc.lower()

    # Allow SharePoint domains even if they are under excluded Microsoft domains
    if any(allowed in domain for allowed in ALLOWED_DOMAINS):
        return False

    if any(excluded in domain for excluded in EXCLUDED_DOMAINS):
        return True

    if any(microsoft_domain in domain for microsoft_domain in EXCLUDED_MICROSOFT_DOMAINS):
        return True

    return False

def should_take_screenshot(link):
    return not is_excluded_domain(link)

def ensure_http(link):
    parsed = urlparse(link)
    if not parsed.scheme:
        return 'http://' + link
    return link

def connect_to_outlook():
    print("Connecting to Outlook...")
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(USERNAME, PASSWORD)
    return mail

def fetch_latest_email(mail):
    print("Fetching latest email...")
    mail.select("inbox")
    result, data = mail.search(None, 'ALL')
    email_ids = data[0].split()
    if not email_ids:
        return None, None
    latest_email_id = email_ids[-1]
    result, data = mail.fetch(latest_email_id, "(RFC822)")
    return data[0][1], latest_email_id

def get_email_content(raw_email):
    print("Getting email content...")
    msg = email.message_from_bytes(raw_email)
    subject, encoding = decode_header(msg["Subject"])[0]
    if isinstance(subject, bytes):
        subject = subject.decode(encoding if encoding else "utf-8")
    from_ = msg.get("From")
    date_ = msg.get("Date")
    return subject, from_, date_, msg

def print_email(subject, from_):
    print(f"Subject: {subject}")
    print(f"From: {from_}")

def check_for_new_emails(mail, latest_email_id):
    print("Checking for new emails...")
    mail.select("inbox")
    result, data = mail.search(None, 'ALL')
    email_ids = data[0].split()
    new_email_ids = [eid for eid in email_ids if int(eid) > int(latest_email_id)]
    return new_email_ids

def create_folder_with_timestamp(date_):
    print("Creating folder with timestamp...")
    date_obj = email.utils.parsedate_to_datetime(date_)
    cst_timezone = pytz.timezone("America/Chicago")
    cst_date_obj = date_obj.astimezone(cst_timezone)
    folder_name = cst_date_obj.strftime("%Y-%m-%d_%H-%M-%S_CST")
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
    return folder_name

def setup_email_directory(date_):
    print("Setting up email directory...")
    folder_name = create_folder_with_timestamp(date_)
    attachments_folder = os.path.join(folder_name, "attachments")
    os.makedirs(attachments_folder, exist_ok=True)
    # Initialize result files
    result_files = ["links.txt", "VT_links.txt", "VT_files.txt", "VT_results.txt",
                   "analysis.txt", "screenshot_info.txt", "phishing_probabilities.txt", "ocr_results.txt"]
    for file_name in result_files:
        open(os.path.join(folder_name, file_name), 'w').close()
    return folder_name, attachments_folder

def save_attachments(msg, attachments_folder):
    print("Saving attachments...")
    attachment_files = []
    for part in msg.walk():
        if part.get_content_maintype() == 'multipart':
            continue
        content_disposition = str(part.get('Content-Disposition') or '')
        if 'attachment' in content_disposition.lower():
            filename = part.get_filename()
            if filename:
                payload = part.get_payload(decode=True)
                if payload:  # Ensure payload is not None
                    filepath = os.path.join(attachments_folder, filename)
                    with open(filepath, 'wb') as f:
                        f.write(payload)
                    attachment_files.append(filepath)
    return attachment_files

def extract_links(text):
    print("Extracting links from text...")
    extractor = URLExtract()
    links = extractor.find_urls(text)
    return links

def sanitize_link(link):
    # Remove leading/trailing whitespace and special characters
    link = link.strip().strip('<>').strip('\"\'')
    # Remove trailing punctuation
    link = link.rstrip('.,;:')
    # Only add 'http://' if no scheme is present
    if not re.match(r'^[a-zA-Z][a-zA-Z0-9+\-.]*://', link):
        link = 'http://' + link
    return link

def is_valid_and_reachable_url(url):
    try:
        parsed = urlparse(url)
        if not parsed.netloc:
            return False

        if parsed.scheme not in ('http', 'https'):
            # Consider non-HTTP URLs as valid but not reachable
            return True

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                          'AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/85.0.4183.102 Safari/537.36'
        }

        session = requests.Session()
        session.headers.update(headers)

        # First try a HEAD request
        try:
            response = session.head(url, allow_redirects=True, timeout=10)
            if 200 <= response.status_code < 400:
                return True
            else:
                # If HEAD request didn't return 2xx or 3xx, try a GET request
                response = session.get(url, allow_redirects=True, timeout=10)
                return 200 <= response.status_code < 400
        except requests.RequestException:
            # If HEAD request fails, try a GET request
            response = session.get(url, allow_redirects=True, timeout=10)
            return 200 <= response.status_code < 400
    except (requests.RequestException, ValueError) as e:
        print(f"Error checking URL {url}: {e}")
        return False

def save_links_to_file(links, file_path):
    print("Saving links to file...")
    valid_links = []
    sanitized_links = [sanitize_link(link) for link in links]

    # Use a ThreadPoolExecutor to check links concurrently
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        future_to_link = {executor.submit(is_valid_and_reachable_url, link): link for link in sanitized_links}
        with open(file_path, 'a') as file:
            for future in concurrent.futures.as_completed(future_to_link):
                link = future_to_link[future]
                try:
                    is_valid = future.result()
                    if is_valid:
                        file.write(link + '\n')
                        valid_links.append(link)
                        print(f"Scanning link: {link}")
                    # Do not print skipped links
                except Exception as exc:
                    print(f"Error checking link {link}: {exc}")
    return valid_links

def extract_links_from_pdf(file_path):
    print(f"Extracting links from PDF: {file_path}")
    links = []

    # Extract text from PDF
    reader = PdfReader(file_path)
    text_content = ""
    for page in reader.pages:
        text = page.extract_text()
        if text:
            text_content += text
            links.extend(extract_links(text))

    # Extract QR codes from PDF
    images = extract_images_from_pdf(file_path)
    for image in images:
        qr_links = scan_qr_codes_from_image(image)
        links.extend(qr_links)

    return links

def extract_images_from_pdf(file_path):
    images = []
    try:
        from pdf2image import convert_from_path
        images = convert_from_path(file_path)
    except Exception as e:
        print(f"Failed to extract images from PDF {file_path}: {e}")
    return images

def extract_links_from_docx(file_path):
    print(f"Extracting links from DOCX: {file_path}")
    links = []
    doc = docx.Document(file_path)
    text_content = ""
    for paragraph in doc.paragraphs:
        text_content += paragraph.text + "\n"
        links.extend(extract_links(paragraph.text))

    # Extract QR codes from images in DOCX
    images = extract_images_from_docx(file_path)
    for image in images:
        qr_links = scan_qr_codes_from_image(image)
        links.extend(qr_links)

    return links

def extract_images_from_docx(file_path):
    images = []
    try:
        from docx import Document
        doc = Document(file_path)
        for rel in doc.part.rels.values():
            if isinstance(rel._target, docx.parts.image.ImagePart):
                image_bytes = rel._target.blob
                image = Image.open(io.BytesIO(image_bytes))
                images.append(image)
    except Exception as e:
        print(f"Failed to extract images from DOCX {file_path}: {e}")
    return images

def extract_links_from_txt(file_path):
    print(f"Extracting links from TXT: {file_path}")
    links = []
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
        links.extend(extract_links(text))

    # Extract QR codes from images in TXT (unlikely but possible if embedded)
    # Skipping image extraction from TXT as it's plain text
    return links

def extract_links_from_html_file(file_path):
    print(f"Extracting links from HTML file: {file_path}")
    links = []
    with open(file_path, 'r', encoding='utf-8') as file:
        html_content = file.read()
        links.extend(extract_links_from_html_content(html_content))
    return links

def extract_links_from_attachment(file_path):
    print(f"Extracting links from attachment: {file_path}")
    links = []
    if file_path.lower().endswith('.pdf'):
        links.extend(extract_links_from_pdf(file_path))
    elif file_path.lower().endswith('.docx') or file_path.lower().endswith('.doc'):
        links.extend(extract_links_from_docx(file_path))
    elif file_path.lower().endswith('.txt') or file_path.lower().endswith('.rtf'):
        links.extend(extract_links_from_txt(file_path))
    elif file_path.lower().endswith('.html') or file_path.lower().endswith('.htm'):
        links.extend(extract_links_from_html_file(file_path))
    else:
        # For other file types, attempt to perform OCR if it's an image
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image)
            links.extend(extract_links(text))
            # Scan QR codes
            qr_links = scan_qr_codes_from_image(image)
            links.extend(qr_links)
        except Exception as e:
            print(f"Failed to extract links from {file_path}: {e}")
    return links

def scan_qr_codes_from_image(image):
    print(f"Scanning QR codes in image")
    qr_links = []
    try:
        decoded_objects = qr_decode(image)
        for obj in decoded_objects:
            qr_link = obj.data.decode("utf-8")
            print(f"Found QR code link: {qr_link}")
            qr_links.append(qr_link)
    except Exception as e:
        print(f"Failed to scan QR code in image: {e}")
    return qr_links

def scan_links_with_vt(links, folder_name):
    print("Scanning links with VirusTotal...")
    url = f"{VT_API_URL}/urls"
    headers = {
        'x-apikey': VT_API_KEY,
        'accept': 'application/json'
    }

    def scan_link(link):
        if is_excluded_domain(link):
            # Skip scanning excluded domains
            return link, None

        try:
            params = {'url': link}
            response = requests.post(url, data=params, headers=headers)
            response.raise_for_status()  # Check for HTTP errors
            analysis_id = response.json().get('data', {}).get('id')
            if analysis_id:
                # Polling until the scan is complete
                analysis_url = f"{VT_API_URL}/analyses/{analysis_id}"
                while True:
                    analysis_response = requests.get(analysis_url, headers=headers)
                    analysis_response.raise_for_status()
                    json_response = analysis_response.json()
                    status = json_response.get('data', {}).get('attributes', {}).get('status')
                    if status == 'completed':
                        break
                    print(f"Waiting for scan to complete for: {link}")
                    time.sleep(10)  # Wait for 10 seconds before checking again

                return link, json_response
            else:
                print(f"Failed to retrieve analysis ID for: {link}")
                return link, None

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                print(f"API limit reached for VirusTotal, skipping link: {link}")
            else:
                print(f"Error scanning link {link} with VirusTotal: {e}")
            return link, None

    total_malicious_links = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_link = {executor.submit(scan_link, link): link for link in links}
        with open(os.path.join(folder_name, 'VT_links.txt'), 'w') as f_vt:
            for future in concurrent.futures.as_completed(future_to_link):
                link = future_to_link[future]
                try:
                    link, result = future.result()
                    if result:
                        f_vt.write(f"Scan result for: {link}\n")
                        f_vt.write(f"{json.dumps(result, indent=4)}\n\n")
                        malicious_count = result.get('data', {}).get('attributes', {}).get('stats', {}).get('malicious', 0)
                        if malicious_count > 0:
                            total_malicious_links += 1
                    else:
                        f_vt.write(f"Failed to scan link: {link}\n\n")
                except Exception as exc:
                    print(f'Link {link} generated an exception: {exc}')
                    f_vt.write(f"Failed to scan link: {link}\nError: {exc}\n\n")

    # Write total malicious links to VT_results.txt
    with open(os.path.join(folder_name, 'VT_results.txt'), 'a') as f_result:
        f_result.write(f"Total Malicious Links Found: {total_malicious_links}\n")

def scan_files_with_vt(attachment_files, folder_name):
    print("Scanning files with VirusTotal...")
    url = f"{VT_API_URL}/files"
    headers = {
        'x-apikey': VT_API_KEY,
    }

    def scan_file(file_path):
        # Check file size limit (VirusTotal API free tier limit is 32MB)
        if os.path.getsize(file_path) > 32 * 1024 * 1024:
            print(f"File {file_path} is larger than 32MB and cannot be scanned.")
            return file_path, None

        # Exclude certain file types if necessary
        if file_path.lower().endswith(('.eml', '.msg')):
            print(f"Skipping scan for email file: {file_path}")
            return file_path, None

        try:
            with open(file_path, 'rb') as f:
                files = {'file': (os.path.basename(file_path), f)}
                response = requests.post(url, files=files, headers=headers)
                response.raise_for_status()
                analysis_id = response.json().get('data', {}).get('id')
                if analysis_id:
                    # Polling until the scan is complete
                    analysis_url = f"{VT_API_URL}/analyses/{analysis_id}"
                    while True:
                        analysis_response = requests.get(analysis_url, headers=headers)
                        analysis_response.raise_for_status()
                        json_response = analysis_response.json()
                        status = json_response.get('data', {}).get('attributes', {}).get('status')
                        if status == 'completed':
                            break
                        print(f"Waiting for scan to complete for: {file_path}")
                        time.sleep(10)  # Wait for 10 seconds before checking again

                    return file_path, json_response
                else:
                    print(f"Failed to retrieve analysis ID for: {file_path}")
                    return file_path, None

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                print(f"API limit reached for VirusTotal, skipping file: {file_path}")
            else:
                print(f"Error scanning file {file_path} with VirusTotal: {e}")
            return file_path, None
        except Exception as e:
            print(f"Error scanning file {file_path} with VirusTotal: {e}")
            return file_path, None

    total_malicious_files = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_file = {executor.submit(scan_file, file_path): file_path for file_path in attachment_files}
        with open(os.path.join(folder_name, 'VT_files.txt'), 'w') as f_vt:
            for future in concurrent.futures.as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    file_path, result = future.result()
                    if result:
                        f_vt.write(f"Scan result for: {file_path}\n")
                        f_vt.write(f"{json.dumps(result, indent=4)}\n\n")
                        malicious_count = result.get('data', {}).get('attributes', {}).get('stats', {}).get('malicious', 0)
                        if malicious_count > 0:
                            total_malicious_files += 1
                    else:
                        f_vt.write(f"Failed to scan file: {file_path}\n\n")
                except Exception as exc:
                    print(f'File {file_path} generated an exception: {exc}')
                    f_vt.write(f"Failed to scan file: {file_path}\nError: {exc}\n\n")

    # Write total malicious files to VT_results.txt
    with open(os.path.join(folder_name, 'VT_results.txt'), 'a') as f_result:
        f_result.write(f"Total Malicious Files Found: {total_malicious_files}\n")

def take_screenshot(driver, link, screenshots_folder, folder_name):
    """
    Takes screenshots of the given link and detects phishing phrases within them.

    Args:
        driver (webdriver): Selenium WebDriver instance.
        link (str): URL to take screenshots of.
        screenshots_folder (str): Directory to save screenshots.
        folder_name (str): Directory to save detection results.

    Returns:
        list: Paths to the saved screenshots.
    """
    screenshot_paths = []
    try:
        link = ensure_http(link)
        driver.get(link)
        time.sleep(5)  # Wait for 5 seconds to ensure the page loads

        # Get the final URL after all redirects
        final_url = driver.current_url
        parsed_url = urlparse(final_url)
        domain = parsed_url.netloc.lower()
        if domain.startswith("www."):
            domain = domain[4:]

        if is_excluded_domain(final_url):
            # Skip taking screenshot for excluded domains
            return screenshot_paths

        print(f"Scanning link: {final_url}")

        # Get the window height and page height
        window_height = driver.execute_script('return window.innerHeight')
        page_height = driver.execute_script("return document.body.scrollHeight")
        num_screenshots = max(1, page_height // window_height)

        for i in range(num_screenshots):
            driver.execute_script(f"window.scrollTo(0, {i * window_height});")
            # Name screenshot after domain
            screenshot_filename = f"{domain}_{i+1}.png"
            screenshot_path = os.path.join(screenshots_folder, screenshot_filename)
            driver.save_screenshot(screenshot_path)
            screenshot_paths.append(screenshot_path)
    except Exception as e:
        print(f"Failed to take screenshot of {link}: {e}")
    return screenshot_paths

def crawl_links(driver, link, visited_links, link_hierarchy, domain_crawl_count):
    new_links = []
    try:
        link = ensure_http(link)
        driver.get(link)
        time.sleep(5)  # Wait for 5 seconds to ensure the page loads

        # Get the final URL after all redirects
        final_url = driver.current_url
        final_domain = urlparse(final_url).netloc.lower()

        if final_domain.startswith("www."):
            final_domain = final_domain[4:]

        if is_excluded_domain(final_url):
            # Skip crawling for excluded domains
            return new_links

        # Determine if the current domain is trusted
        is_trusted = is_trusted_domain(final_domain)

        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        anchors = soup.find_all('a', href=True)
        for anchor in anchors:
            new_link = anchor['href']
            new_link = urljoin(final_url, new_link)  # Resolve relative URLs
            domain = urlparse(new_link).netloc.lower()
            if domain.startswith("www."):
                domain = domain[4:]

            if is_trusted:
                # If current page is trusted, only consider links to non-trusted domains
                if is_trusted_domain(domain):
                    continue  # Skip links to other trusted domains
            else:
                # If current page is not trusted, skip links to trusted domains
                if is_trusted_domain(domain):
                    continue  # Skip links to trusted domains

            if is_excluded_domain(new_link):
                continue  # Skip excluded domains

            if domain_crawl_count[domain] >= MAX_LINKS_PER_DOMAIN:
                continue
            if new_link not in visited_links and len(new_links) < CRAWL_LINK_LIMIT:
                if is_valid_and_reachable_url(new_link):
                    visited_links.add(new_link)
                    new_links.append(new_link)
                    link_hierarchy[new_link] = final_url
                    domain_crawl_count[domain] += 1
            if len(new_links) >= CRAWL_LINK_LIMIT:
                break  # Stop crawling further if we've reached the limit
    except Exception as e:
        print(f"Failed to crawl link {link}: {e}")
    return new_links

def crawl_and_screenshot_links(links, folder_name):
    print("Taking screenshots and crawling links...")
    screenshots_folder = os.path.join(folder_name, "screenshots")
    os.makedirs(screenshots_folder, exist_ok=True)

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.page_load_strategy = 'normal'  # Wait for full page load

    # Set a common user-agent
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/85.0.4183.102 Safari/537.36")

    # Disable automation flags
    options.add_experimental_option('useAutomationExtension', False)
    options.add_experimental_option('excludeSwitches', ['enable-automation'])

    driver = webdriver.Chrome(options=options, service=Service(ChromeDriverManager().install()))

    # Additional settings to make headless Chrome more like a real browser
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined})
        '''
    })

    all_new_links = []
    screenshot_data = {}

    # Initialize tracking structures outside the loop
    visited_links = set()
    link_hierarchy = {}
    domain_crawl_count = defaultdict(int)

    for link in links:
        if is_excluded_domain(link):
            continue  # Skip excluded domains

        if should_take_screenshot(link) and is_valid_and_reachable_url(link):
            # Check if the link has already been visited
            if link not in visited_links:
                screenshots = take_screenshot(driver, link, screenshots_folder, folder_name)
                if screenshots:
                    screenshot_data[link] = screenshots
                    visited_links.add(link)

                    # Crawl links within the page
                    new_links = crawl_links(driver, link, visited_links, link_hierarchy, domain_crawl_count=domain_crawl_count)
                    all_new_links.extend(new_links)
            else:
                continue  # Already visited
        else:
            continue  # Skip invalid or unreachable link

    driver.quit()

    # Save crawled links to the links file
    valid_new_links = save_links_to_file(all_new_links, os.path.join(folder_name, "links.txt"))

    return screenshot_data, valid_new_links

def perform_ocr_on_screenshots(screenshot_data, folder_name):
    print("Performing OCR on screenshots...")
    ocr_folder = os.path.join(folder_name, "ocr_results")
    os.makedirs(ocr_folder, exist_ok=True)
    ocr_results = []
    detected_phish_links = set()

    for link, screenshots in screenshot_data.items():
        for screenshot_path in screenshots:
            try:
                image = Image.open(screenshot_path)
                text = pytesseract.image_to_string(image)
                ocr_results.append((screenshot_path, text))
                ocr_file_path = os.path.join(ocr_folder, f"{os.path.basename(screenshot_path)}.txt")
                with open(ocr_file_path, 'w') as f:
                    f.write(text)

                # Search for phishing phrases
                if search_phish_phrases(text):
                    detected_phish_links.add(link)
                    print(f"Phishing phrase detected in {screenshot_path}")
            except Exception as e:
                print(f"Failed to perform OCR on {screenshot_path}: {e}")

    # Save all OCR results in a single file
    all_ocr_results_file_path = os.path.join(ocr_folder, "all_ocr_results.txt")
    with open(all_ocr_results_file_path, 'w') as f:
        for screenshot_path, text in ocr_results:
            f.write(f"Screenshot: {screenshot_path}\n")
            f.write(f"Text:\n{text}\n")
            f.write("="*50 + "\n")

    return detected_phish_links

def perform_ocr_on_attachments(attachment_files, folder_name, valid_links):
    print("Performing OCR on attachments...")
    ocr_folder = os.path.join(folder_name, "ocr_results")
    os.makedirs(ocr_folder, exist_ok=True)
    ocr_results = []
    detected_phish_links = set()

    for file_path in attachment_files:
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image)
            ocr_results.append((file_path, text))
            ocr_file_path = os.path.join(ocr_folder, f"{os.path.basename(file_path)}.txt")
            with open(ocr_file_path, 'w') as f:
                f.write(text)

            # Search for phishing phrases
            if search_phish_phrases(text):
                # Associate with all valid links
                for link in valid_links:
                    detected_phish_links.add(link)
                print(f"Phishing phrase detected in attachment {file_path}")
        except Exception as e:
            print(f"Failed to perform OCR on {file_path}: {e}")

    # Save all OCR results in a single file
    all_ocr_results_file_path = os.path.join(ocr_folder, "all_ocr_results.txt")
    with open(all_ocr_results_file_path, 'a') as f:
        for file_path, text in ocr_results:
            f.write(f"Attachment: {file_path}\n")
            f.write(f"Text:\n{text}\n")
            f.write("="*50 + "\n")

    return detected_phish_links

def write_analysis_file(folder_name, from_, subject, body, link_hierarchy, valid_links):
    print("Writing analysis file...")
    analysis_file_path = os.path.join(folder_name, "analysis.txt")
    with open(analysis_file_path, 'w') as f:
        f.write(f"From: {from_}\n")
        f.write(f"Subject: {subject}\n")
        f.write("Body:\n")
        f.write(body)
        f.write("\nLink Hierarchy:\n")
        for link, parent_link in link_hierarchy.items():
            f.write(f"{link} (Parent: {parent_link})\n")

    # Search for phishing phrases in the analysis
    combined_content = body.lower()
    if search_phish_phrases(combined_content):
        # Associate with all valid links
        detected_phish_links = set(valid_links)
        print("Phishing phrase detected in analysis file.")
        return detected_phish_links
    return set()

# ==========================================
# ============ GRAPH API FUNCTIONS ==========
# ==========================================

def get_access_token(client_id, client_secret, tenant_id):
    authority = f'https://login.microsoftonline.com/{tenant_id}'
    app = msal.ConfidentialClientApplication(
        client_id, authority=authority, client_credential=client_secret
    )
    scopes = ["https://graph.microsoft.com/.default"]
    result = app.acquire_token_silent(scopes, account=None)
    if not result:
        result = app.acquire_token_for_client(scopes=scopes)

    if 'access_token' in result:
        return result['access_token']
    else:
        print(f"Error acquiring token: {result.get('error_description')}")
        return None

def download_eml_with_graph_api(access_token, recipient_email, sender_email, folder_name):
    print("Connecting to Microsoft Graph API...")
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }

    # Define the query parameters
    params = {
        "$top": 10,  # Fetch the most recent 10 emails to filter manually
        "$orderby": "receivedDateTime desc"
    }

    # Use the recipient's email in the endpoint
    endpoint = f"https://graph.microsoft.com/v1.0/users/{recipient_email}/messages"

    response = requests.get(endpoint, headers=headers, params=params)

    if response.status_code == 200:
        messages = response.json().get("value", [])
        if messages:
            # Filter the messages by the sender's email
            for email_data in messages:
                if email_data.get("from").get("emailAddress").get("address").lower() == sender_email.lower():
                    email_id = email_data.get("id")
                    subject = email_data.get("subject")
                    print(f"Found matching email: {subject}")

                    # Now retrieve the .eml file
                    eml_endpoint = f"https://graph.microsoft.com/v1.0/users/{recipient_email}/messages/{email_id}/$value"
                    eml_response = requests.get(eml_endpoint, headers=headers)

                    if eml_response.status_code == 200:
                        eml_filename = f"{subject}.eml".replace(" ", "_").replace("/", "_")
                        eml_file_path = os.path.join(folder_name, eml_filename)
                        with open(eml_file_path, "wb") as eml_file:
                            eml_file.write(eml_response.content)
                        print(f".eml file downloaded and saved to: {eml_file_path}")
                        return eml_file_path
                    else:
                        print(f"Error fetching .eml file: {eml_response.status_code} - {eml_response.text}")
                        return None

            print("No emails found from the specified sender in the recipient's inbox.")
        else:
            print("No emails found in the specified inbox.")
    else:
        print(f"Error fetching messages: {response.status_code} - {response.text}")
    return None

# ==========================================
# ============ EMAIL SENDING =================
# ==========================================

def send_email_with_analysis(recipient_email, subject, body, folder_name):
    msg = MIMEMultipart()
    msg['From'] = USERNAME
    msg['To'] = recipient_email
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))

    # Attach the ZIP file
    zip_filename = f"{folder_name}.zip"
    shutil.make_archive(folder_name, 'zip', folder_name)
    zip_filepath = f"{folder_name}.zip"

    with open(zip_filepath, 'rb') as f:
        part = MIMEApplication(f.read(), Name=os.path.basename(zip_filepath))
        part['Content-Disposition'] = f'attachment; filename="{os.path.basename(zip_filepath)}"'
        msg.attach(part)

    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.starttls()
            server.login(USERNAME, PASSWORD)
            server.sendmail(USERNAME, recipient_email, msg.as_string())
        print(f"Analysis results sent to {recipient_email}")
    except Exception as e:
        print(f"Failed to send email with analysis results: {e}")

# ==========================================
# ============ OCR FUNCTIONS =================
# ==========================================

def perform_ocr_on_screenshots(screenshot_data, folder_name):
    print("Performing OCR on screenshots...")
    ocr_folder = os.path.join(folder_name, "ocr_results")
    os.makedirs(ocr_folder, exist_ok=True)
    ocr_results = []
    detected_phish_links = set()

    for link, screenshots in screenshot_data.items():
        for screenshot_path in screenshots:
            try:
                image = Image.open(screenshot_path)
                text = pytesseract.image_to_string(image)
                ocr_results.append((screenshot_path, text))
                ocr_file_path = os.path.join(ocr_folder, f"{os.path.basename(screenshot_path)}.txt")
                with open(ocr_file_path, 'w') as f:
                    f.write(text)

                # Search for phishing phrases
                if search_phish_phrases(text):
                    detected_phish_links.add(link)
                    print(f"Phishing phrase detected in {screenshot_path}")
            except Exception as e:
                print(f"Failed to perform OCR on {screenshot_path}: {e}")

    # Save all OCR results in a single file
    all_ocr_results_file_path = os.path.join(ocr_folder, "all_ocr_results.txt")
    with open(all_ocr_results_file_path, 'w') as f:
        for screenshot_path, text in ocr_results:
            f.write(f"Screenshot: {screenshot_path}\n")
            f.write(f"Text:\n{text}\n")
            f.write("="*50 + "\n")

    return detected_phish_links

def perform_ocr_on_attachments(attachment_files, folder_name, valid_links):
    print("Performing OCR on attachments...")
    ocr_folder = os.path.join(folder_name, "ocr_results")
    os.makedirs(ocr_folder, exist_ok=True)
    ocr_results = []
    detected_phish_links = set()

    for file_path in attachment_files:
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image)
            ocr_results.append((file_path, text))
            ocr_file_path = os.path.join(ocr_folder, f"{os.path.basename(file_path)}.txt")
            with open(ocr_file_path, 'w') as f:
                f.write(text)

            # Search for phishing phrases
            if search_phish_phrases(text):
                # Associate with all valid links
                for link in valid_links:
                    detected_phish_links.add(link)
                print(f"Phishing phrase detected in attachment {file_path}")
        except Exception as e:
            print(f"Failed to perform OCR on {file_path}: {e}")

    # Save all OCR results in a single file
    all_ocr_results_file_path = os.path.join(ocr_folder, "all_ocr_results.txt")
    with open(all_ocr_results_file_path, 'a') as f:
        for file_path, text in ocr_results:
            f.write(f"Attachment: {file_path}\n")
            f.write(f"Text:\n{text}\n")
            f.write("="*50 + "\n")

    return detected_phish_links

# ==========================================
# ============ PROCESSING EMAILS ============
# ==========================================

def process_email(eid, mail, latest_email_id):
    result, data = mail.fetch(eid, "(RFC822)")
    raw_email = data[0][1]
    subject, from_, date_, msg = get_email_content(raw_email)
    print_email(subject, from_)
    folder_name, attachments_folder = setup_email_directory(date_)
    msg = email.message_from_bytes(raw_email)
    attachment_files = save_attachments(msg, attachments_folder)

    # Extract body text and HTML content
    body, html_content, images = extract_body_text(msg)

    # Extract links from images in the email body
    image_links = []
    for image in images:
        qr_links = scan_qr_codes_from_image(image)
        image_links.extend(qr_links)

    # Check if the email contains the specific phrase indicating an alert
    combined_content = body + "\n" + html_content
    if "An informational alert has been triggered" in combined_content:
        print("Detected 'An informational alert has been triggered' in the email body.")
        recipient_email_match = re.search(r'to ([\w\.-]+@westernexp\.com)', combined_content)
        sender_email_match = re.search(r'sent by ([\w\.-]+@[\w\.-]+)', combined_content)

        if recipient_email_match and sender_email_match:
            recipient_email = recipient_email_match.group(1)
            sender_email = sender_email_match.group(1)

            print(f"Extracted recipient email: {recipient_email}")
            print(f"Extracted sender email: {sender_email}")

            # Get the access token and download the .eml file
            access_token = get_access_token(CLIENT_ID, CLIENT_SECRET, TENANT_ID)
            eml_file_path = download_eml_with_graph_api(access_token, recipient_email, sender_email, folder_name)

            if eml_file_path:
                print(f"Processing the downloaded .eml file: {eml_file_path}")
                with open(eml_file_path, 'rb') as f:
                    eml_content = f.read()
                subject, from_, date_, eml_msg = get_email_content(eml_content)
                attachment_files.extend(save_attachments(eml_msg, attachments_folder))

                # Extract body text and HTML content from the downloaded email
                body, html_content, images = extract_body_text(eml_msg)

                # Extract links from images in the downloaded email body
                for image in images:
                    qr_links = scan_qr_codes_from_image(image)
                    image_links.extend(qr_links)

    # Step 1: Extract all links from plain text and HTML content
    links = extract_links(body)
    html_links = extract_links_from_html_content(html_content)
    all_extracted_links = links + html_links + image_links

    # Debugging: Print extracted links before sanitization
    print(f"Extracted {len(all_extracted_links)} links from email body and HTML content.")
    # Optional: Remove or keep this print if too verbose

    # Sanitize links
    sanitized_links = [sanitize_link(link) for link in all_extracted_links]

    # Debugging: Print sanitized links
    # Optional: Remove or keep this print if too verbose

    # Save and get valid links
    links_file_path = os.path.join(folder_name, "links.txt")
    valid_links = save_links_to_file(sanitized_links, links_file_path)

    # Process attachments, including nested .eml files
    attachment_links = []
    process_attachments(attachment_files, attachments_folder, attachment_links)

    # Sanitize attachment links
    sanitized_attachment_links = [sanitize_link(link) for link in attachment_links]

    # Save and get valid attachment links
    valid_attachment_links = save_links_to_file(sanitized_attachment_links, links_file_path)

    # Combine all valid links
    all_links = valid_links + valid_attachment_links

    # Scan attachments with VirusTotal
    scan_files_with_vt(attachment_files, folder_name)

    # Step 2: Crawl and take screenshots of all extracted links
    screenshot_data, new_links = crawl_and_screenshot_links(all_links, folder_name)

    # Add new links to the text file
    valid_new_links = save_links_to_file(new_links, links_file_path)
    all_links += valid_new_links

    # Step 3: Run AI phishing detection on all valid links
    phishing_probabilities_file_path = os.path.join(folder_name, "phishing_probabilities.txt")
    with open(phishing_probabilities_file_path, 'w') as f_probabilities:
        for link in all_links:
            phishing_risk = predict_phishing(link)
            f_probabilities.write(f"URL: {link}, Phishing Probability: {phishing_risk:.2f}%\n")

    # Step 4: Scan links with VirusTotal
    scan_links_with_vt(all_links, folder_name)

    # Step 5: Perform OCR on screenshots and save the information
    detected_phish_links_screenshots = perform_ocr_on_screenshots(screenshot_data, folder_name)

    # Step 6: Perform OCR on image attachments and save the information
    detected_phish_links_attachments = perform_ocr_on_attachments(attachment_files, folder_name, valid_links)

    # Step 7: Write analysis file and detect phishing phrases
    detected_phish_links_analysis = write_analysis_file(folder_name, from_, subject, body + "\n" + html_content, {}, valid_links)

    # Combine all detected phish links
    all_detected_phish_links = detected_phish_links_screenshots.union(detected_phish_links_attachments).union(detected_phish_links_analysis)

    # Step 8: Create PHISH FOUND!.txt if any phishing links are detected
    if all_detected_phish_links:
        phish_found_file = os.path.join(folder_name, "PHISH FOUND!.txt")
        with open(phish_found_file, 'w') as f_phish:
            for phish_link in all_detected_phish_links:
                f_phish.write(f"{phish_link}\n")
        print(f"Phishing detected! Details written to {phish_found_file}")
    else:
        print("No phishing phrases detected.")

    # Step 9: Email the analysis results with ZIP attachment
    send_email_with_analysis("cwillis@westernexp.com", f"Analysis of Email: {subject}", body + "\n" + html_content, folder_name)

    return eid

def process_attachments(attachment_files, attachments_folder, attachment_links, depth=0, max_depth=5):
    if depth > max_depth:
        print("Maximum recursion depth reached for attachments")
        return

    for file_path in attachment_files:
        if file_path.lower().endswith('.eml'):
            with open(file_path, 'rb') as f:
                eml_content = f.read()
            subject, from_, date_, eml_msg = get_email_content(eml_content)

            # Extract body text and HTML content
            body, html_content, images = extract_body_text(eml_msg)

            # Extract links from both plain text and HTML content
            extracted_links = extract_links(body)
            html_links = extract_links_from_html_content(html_content)
            all_extracted_links = extracted_links + html_links

            # Extract links from images in the email body
            for image in images:
                qr_links = scan_qr_codes_from_image(image)
                all_extracted_links.extend(qr_links)

            # Sanitize links
            sanitized_links = [sanitize_link(link) for link in all_extracted_links]
            attachment_links.extend(sanitized_links)

            # Save attachments from the nested email
            nested_attachment_files = save_attachments(eml_msg, attachments_folder)
            # Recursively process nested attachments
            process_attachments(nested_attachment_files, attachments_folder, attachment_links, depth + 1, max_depth)
        else:
            # Process other attachment types
            extracted_links = extract_links_from_attachment(file_path)
            sanitized_links = [sanitize_link(link) for link in extracted_links]
            attachment_links.extend(sanitized_links)

def extract_body_text(msg):
    print("Extracting email body text...")
    body = ""
    html_content = ""
    images = []
    if msg.is_multipart():
        for part in msg.walk():
            if part.get_content_maintype() == 'multipart':
                continue
            content_type = part.get_content_type()
            content_disposition = str(part.get('Content-Disposition') or '')
            if 'attachment' in content_disposition.lower():
                continue
            charset = part.get_content_charset() or 'utf-8'
            payload = part.get_payload(decode=True)
            if payload:
                try:
                    decoded_payload = payload.decode(charset, errors='replace')
                except (LookupError, UnicodeDecodeError):
                    decoded_payload = payload.decode('utf-8', errors='replace')
                if content_type == 'text/plain':
                    body += decoded_payload
                elif content_type == 'text/html':
                    html_content += decoded_payload
                elif content_type.startswith('image/'):
                    # Extract images from the email body
                    try:
                        image = Image.open(io.BytesIO(payload))
                        images.append(image)
                    except Exception as e:
                        print(f"Failed to process image in email body: {e}")
    else:
        content_type = msg.get_content_type()
        charset = msg.get_content_charset() or 'utf-8'
        payload = msg.get_payload(decode=True)
        if payload:
            try:
                decoded_payload = payload.decode(charset, errors='replace')
            except (LookupError, UnicodeDecodeError):
                decoded_payload = payload.decode('utf-8', errors='replace')
            if content_type == 'text/plain':
                body = decoded_payload
            elif content_type == 'text/html':
                html_content = decoded_payload
            elif content_type.startswith('image/'):
                # Extract images from the email body
                try:
                    image = Image.open(io.BytesIO(payload))
                    images.append(image)
                except Exception as e:
                    print(f"Failed to process image in email body: {e}")
    return body, html_content, images

def extract_links_from_html_content(html_content):
    print("Extracting links from HTML content...")
    soup = BeautifulSoup(html_content, 'html.parser')
    links = []

    # Extract from <a> tags
    for a_tag in soup.find_all('a', href=True):
        links.append(a_tag['href'])

    # Extract from <img> tags
    for img_tag in soup.find_all('img', src=True):
        links.append(img_tag['src'])

    # Extract from <form> tags
    for form_tag in soup.find_all('form', action=True):
        links.append(form_tag['action'])

    # Extract from inline scripts
    for script_tag in soup.find_all('script'):
        script_content = script_tag.string
        if script_content:
            links.extend(extract_links(script_content))

    # Also extract links from the text content of the HTML
    text_links = extract_links(soup.get_text())
    links.extend(text_links)

    return links

def main():
    print("Starting script...")
    mail = connect_to_outlook()
    raw_email, latest_email_id = fetch_latest_email(mail)
    if raw_email and latest_email_id:
        subject, from_, date_, msg = get_email_content(raw_email)
        print_email(subject, from_)

        latest_email_id = int(latest_email_id)

        while True:
            print("Checking for new emails...")
            new_email_ids = check_for_new_emails(mail, latest_email_id)
            if not new_email_ids:
                print("No new emails. Waiting...")
                time.sleep(10)
                continue

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future_to_email = {executor.submit(process_email, eid, mail, latest_email_id): eid for eid in new_email_ids}
                for future in concurrent.futures.as_completed(future_to_email):
                    eid = future_to_email[future]
                    try:
                        result = future.result()
                        latest_email_id = max(latest_email_id, int(result))
                    except Exception as exc:
                        print(f'Email {eid} generated an exception: {exc}')
            time.sleep(10)
    else:
        print("No emails found in the inbox.")

if __name__ == "__main__":
    main()
